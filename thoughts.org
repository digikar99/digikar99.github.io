#+HTML_HEAD: <meta charset="utf-8">
#+HTML_HEAD: <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="others.css">
#+OPTIONS: toc:nil num:nil html-postamble:nil
#+TITLE: AGI and Cognitive Science

#+BEGIN_CENTER
[[file:index.html#home][Home]] | [[file:common-lisp-and-emacs.html][Common Lisp and Emacs]] | *AI AND COGSCI* | [[file:books-blog.html][Books and Blogs]] | [[./digimon.html][Digimon]]
#+END_CENTER

#+TOC: headlines 2

The quest for intelligence drove me, first to computer science, and then to cognitive science. Before the industrial revolution, physical prowess was important. Since then, machines have been replacing humans for purposes of physical labour. In the 21st century, intellectual prowess remains important. It will be interesting to see intellectual labour being replaced by machines over the upcoming century. Barring networking and connections, an information-processing revolution might be a great equalizer.

As far as definitions go, I have found Dr. Pei Wang's definition of intelligence to be a notable one: /Intelligence is the capacity of an information-processing system to adapt to its environment while operating with insufficient knowledge and resources./ (See [[citet:&Wang2019]].)

For me, sufficiently general intelligence is a means for an alternative substrate for life. So, I have come to want a more concrete definition. I think the programming attitude, "Hey, we can always tweak the program if it doesn't work the way we want it to," or "Hey, let's see if the machine can do this task; we can always build and run another program for another task," is itself an impediment to the development of machines with sufficiently general intelligence. This attitude puts the responsibility of the machine in the hands of the programmers, the machine is no more than a tool for the programmer, and the programmer can get away by building a sloppy not-really-autonomous system. Thus, a definition I find acceptable is:

#+begin_quote
An information processing system may be said to have sufficient general intelligence or autonomy when (i) it can operate a physical body (ii) to be a good-enough domain-general researcher-explorer (iii) for a period not less than two decades (iv) without any direct involvement of the programmer.
#+end_quote

Given the very long term evaluation frame of this project, end-to-end sufficiently general intelligence is unpursuable as a research project in itself. Perhaps, in a world where we look towards machines as tools, as "How can this machine help me (others) achieve what I (they) want to?", research on sufficiently general intelligence might even be unmotivated [[cite:&Herrera2016]], outside a few areas like space exploration.

The definition leads to a number of implications, each of which has the potential to turn into a research project or program.

1. Operating a physical body requires the system to handle high dimensional data from its sensors.
2. Non-involvement of programmers means that the machines should be able to communicate with other humans and machines through its sensors. This leads us to ponder over how infants and toddlers acquire this very important skill of communicating with others in the first few years of their life. This directly leads us to Learning via Shared Attention and Theory of Mind.
3. Being a good-enough domain-general researcher-explorer requires the system to discover causal models of the world from high dimensional data. This precisely is Causal Representation Learning.

Thinking about how sufficiently general intelligence may be achieved has led me to:

- [[file:ai-cgs/baby.html][Baby AGI and Developmental Cognition]]
  1. [[file:ai-cgs/shared-attention.html][Learning via Shared Attention and Theory of Mind]]
  2. [[file:ai-cgs/causality.html][Causal Representation Learning]]
- [[file:ai-cgs/goal-dep-cat.html][Goal-dependent categorization]]

However, after recently going through [[citet:&dreyfus1992]] and [[citet:&dreyfus2007]], the above views might need significant changes given the non-representational stance that Dreyfus takes. 

** Don't we already have AGI with GPT4 and DALL-E?

Empirically, the day an AI system is successful in supervising critical systems including cars on Indian roads, operation rooms, nuclear facilities - a separate system for each is acceptable(!) -  for decades on end without programmer supervision, that will perhaps be the day I will accept that we have an AGI. So long as there is a programmer in some part of the loop of maintaining an AI system, we don't have an AGI. The involvement of the programmer in the maintenance of a long-running computer system essentially gives the AI the freedom to appear to be intelligent without good-enough models of the world.

Until then, we will have AI systems that will have huge commercial success, systems that will solve 80% of our problems, systems that will amplify our power multiplicatively (not additively!). Perhaps, there might not even be any commercial incentive to the development of machines with full general intelligence that provide additive power the way your friends, colleagues, and lab-mates do.

** Are there good reasons to develop an AGI? Wouldn't that be dangerous?

AI is dangerous as it is, in the hands of a super-powerful/wealthy minority or the military. In fact, there already are efforts to decentralize AI. I think developing an AGI doesn't simply mean developing a more capable AI, but rather providing the machine with the means to escape the dangerous purposes (perhaps unintentionally even!) it was designed for.

Besides, even though our brains and minds and bodies are marvelous in certain aspects, they are miserable in some other aspects. Our bodies aren't the best instruments for venturing into outer space. Nor are our disconnected brains the best vessels in the digital age. Entire decades of experience are lost with the death of a human. It takes years to obtain mastery in specific fields.

But equally importantly, perhaps the understanding we obtain during the process of creating an AGI would provide us with better means to understand our own selves. Why do we have any experience at all if we can do all kinds of things during [[https://www.mayoclinic.org/diseases-conditions/sleepwalking/symptoms-causes/syc-20353506][sleeping]]?

** Should I pursue Cognitive Science if I want to pursue AGI?

To the extent that you have never taken courses on philosophy of science, philosophy of mind, I will certainly encourage pursuing Cognitive Science at a place with a strength in philosophical and computational methods.

Another suggestion would also be - as [[https://www.youtube.com/watch?v=pPFSQQ0MUHo][Tomasello has suggested]] - on looking at things through the lens of Ontogeny and Phylogeny. You could pick up his recent publication on [[https://mitpress.mit.edu/9780262047005/the-evolution-of-agency/][The Evolution of Agency]]. A reason is that developing a "baby AGI" - that can bootstrap into an "adult AGI" - seems easier (and safer?) than directly developing an "adult AGI". And when modern day cognitive science or its siblings study cognition, the focus is usually on how the cognition is in adult humans, and I think that makes it hard to separate out which aspects of cognition are inherent to having human level intelligence/consciousness and which aspects are inherent to being /that particular/ human or a human with /that particular background/.

** I have a background in XYZ, can you recommend me something will lead to AGI?

I started out with taking a course on NLP, thinking that understanding language will be sufficient for developing an AGI. That led me to thinking about how human children acquire language without having any language apriori - and thus First Language Acquisition. I got wrapped up in Computational Cognitive Science and Consciousness, because we seem to acquire language in the context of an "internal world" rather than in "complete isolation". In addition, from an evolutionary perspective, prelinguistic cognition feels more primitive than language from an evolutionary or phylogenetic perspective: think about cockroaches or rats.

There are other arguments for "Perception" coming /before/ "Representations", as well as Perception being a necessary condition for AGI. As such, something you can work on includes figuring out how your background relates to perception, as well as how perception integrates into [[file:ai-cgs/nar.html][Non-Axiomatic Reasoning]].

** References
:properties:
:html_container_class: "references"
:end:

bibliography:~/references.bib
